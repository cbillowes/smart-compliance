{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smart Compliance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fraud reduction strategy for compliance and risk management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@4260.045] global loadsave.cpp:248 findDecoder imread_('./raw_data/comparison/clarice_id/clarice_doc_passport_1.jpeg'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.1) /Users/xperience/GHA-OpenCV-Python/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 13\u001b[0m\n\u001b[1;32m      5\u001b[0m id_document \u001b[38;5;241m=\u001b[39m SmartComplianceDocument(cv2\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./raw_data/comparison/clarice_id/clarice_doc_passport_1.jpeg\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# sideways = SmartComplianceDocument(cv2.imread(\"./raw_data/docs/amit_doc_passport_1.png\"))\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# sideways.rotate()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# selfie.crop()\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# id_document.crop()\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[43mid_document\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrotate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m id_document\u001b[38;5;241m.\u001b[39mpreview()\n\u001b[1;32m     15\u001b[0m id_document\u001b[38;5;241m.\u001b[39mextract_characters()\n",
      "File \u001b[0;32m~/code/cbillowes/smart-compliance/domain/core.py:45\u001b[0m, in \u001b[0;36mSmartComplianceDocument.rotate\u001b[0;34m(self, adjust_by, scale)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrotate\u001b[39m(\u001b[38;5;28mself\u001b[39m, adjust_by\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m45\u001b[39m, scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m):\n\u001b[1;32m     44\u001b[0m     adjusted_angle \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhas_detected_at_least_one_face\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m adjusted_angle \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m360\u001b[39m:\n\u001b[1;32m     46\u001b[0m         adjusted_angle \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m adjust_by\n\u001b[1;32m     47\u001b[0m         (h, w) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[0;32m~/code/cbillowes/smart-compliance/domain/core.py:41\u001b[0m, in \u001b[0;36mSmartComplianceDocument.has_detected_at_least_one_face\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhas_detected_at_least_one_face\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/code/cbillowes/smart-compliance/domain/core.py:34\u001b[0m, in \u001b[0;36mSmartComplianceDocument.detect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 34\u001b[0m     gray \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2GRAY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mface_cascade\u001b[38;5;241m.\u001b[39mdetectMultiScale(gray,\n\u001b[1;32m     36\u001b[0m                                               scaleFactor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_factor,\n\u001b[1;32m     37\u001b[0m                                               minNeighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_neighbors,\n\u001b[1;32m     38\u001b[0m                                               minSize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_size)\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.8.1) /Users/xperience/GHA-OpenCV-Python/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    }
   ],
   "source": [
    "from domain.core import SmartComplianceDocument\n",
    "import cv2\n",
    "\n",
    "selfie = SmartComplianceDocument(cv2.imread(\"./raw_data/comparison/amit_passport_1/selfie.jpeg\"))\n",
    "id_document = SmartComplianceDocument(cv2.imread(\"./raw_data/docs/amit_doc_passport_1.png\"))\n",
    "# sideways = SmartComplianceDocument(cv2.imread(\"./raw_data/docs/amit_doc_passport_1.png\"))\n",
    "\n",
    "# sideways.rotate()\n",
    "# sideways.preview()\n",
    "# sideways.has_detected_at_least_one_face()\n",
    "# selfie.crop()\n",
    "# id_document.crop()\n",
    "id_document.rotate()\n",
    "id_document.preview()\n",
    "id_document.extract_characters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from domain.core import SmartCompliance\n",
    "\n",
    "SmartCompliance(selfie, id_document).verify()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selfie with passport/ID\n",
    "\n",
    "Facial detection and recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade_path = \"haarcascade_frontalface_default.xml\"\n",
    "images = []\n",
    "for dirname, _, filenames in os.walk('./data/'):\n",
    "    for filename in filenames:\n",
    "        im = cv2.imread(os.path.join(dirname, filename))\n",
    "        images.append(im)\n",
    "\n",
    "for i, image in enumerate(images):\n",
    "    # #image converted to grayscale\n",
    "    gray=cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # #face detection performed\n",
    "    faceRects=FaceDetector(faceCascadePath=face_cascade_path).detect(gray,scaleFactor=1.1,minNeighbors=5,minSize=(30,30))\n",
    "\n",
    "    #faces drawn\n",
    "    for j, (x,y,w,h) in enumerate(faceRects):\n",
    "        cv2.rectangle(image,(x,y),(x+w,y+h),(0,255,0),2)\n",
    "        cv2.imwrite(f\"./raw_data/detected_{i}_{j}.jpg\",image[y:y+h, x:x+w])\n",
    "\n",
    "    #image shown\n",
    "    plt.imshow(cv2.cvtColor(image,cv2.COLOR_BGR2RGB))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Photo passport/ID only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade_path = \"haarcascade_frontalface_default.xml\"\n",
    "\n",
    "def detect_and_crop(folder, filename, image):\n",
    "  gray=cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "  # #face detection performed\n",
    "  rectangles=FaceDetector(faceCascadePath=face_cascade_path).detect(gray,scaleFactor=1.1,minNeighbors=5,minSize=(30,30))\n",
    "\n",
    "  #faces drawn\n",
    "  for i, (x,y,w,h) in enumerate(rectangles):\n",
    "    cv2.rectangle(image,(x,y),(x+w,y+h),(0,255,0),2)\n",
    "    cv2.imwrite(f\"{folder}/{filename}_{i}.jpg\",image[y:y+h, x:x+w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (os.path.exists(\"./raw_data/output\") == False):\n",
    "  os.mkdir(\"./raw_data/output\")\n",
    "\n",
    "for dirname, _, filenames in os.walk('./raw_data/comparison/'):\n",
    "  for i, filename in enumerate(filenames):\n",
    "    if filename != '.DS_Store':\n",
    "      folder = f\"./raw_data/output/{os.path.basename(dirname)}\"\n",
    "\n",
    "      if (os.path.exists(folder) == False):\n",
    "        os.mkdir(folder)\n",
    "\n",
    "      im = cv2.imread(os.path.join(dirname, filename))\n",
    "      detect_and_crop(folder, i, im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DeepFace.verify(\"./raw_data/output/amit_passport_1/1_0.jpg\", \"./raw_data/output/amit_passport_1/2_1.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1_path = \"\"\n",
    "img2_path = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "  \"VGG-Face\",\n",
    "  \"Facenet\",\n",
    "  \"Facenet512\",\n",
    "  \"OpenFace\",\n",
    "  \"DeepFace\",\n",
    "  \"DeepID\",\n",
    "  \"ArcFace\",\n",
    "  \"Dlib\",\n",
    "  \"SFace\",\n",
    "]\n",
    "\n",
    "for model in models:\n",
    "  result = DeepFace.verify(img1_path=img1_path,\n",
    "      img2_path=img2_path,\n",
    "      model_name = model\n",
    "  )\n",
    "  print(model, result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smart-compliance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
